# Der Trade-off zwischen Effektivität und Differential Privacy bei Vorhersagemodellen mittels maschineller Lernverfahren: Eine Analyse anhand der Kreditwürdigkeitsprüfung im Finanzwesen
#### Bachelorarbeit, Nicolas Ebner, 2353610


## Zusammenfassung

Generative Adversarial Networks (GAN) haben in letzter Zeit zunehmend Aufmerksamkeit erhalten. Ein Grund ist ihre Fähigkeit, realistische Datensätze mit hohem Schutz der Pri-vatsphäre zu generieren (Ma et al. 2020, 1). Bei der Anwendung von GANs auf sensible oder private Trainingsbeispiele, wie zum Beispiel finanziellen Aufzeichnungen, ist es dennoch ein Problem, dass persönliche Informationen von Personen preisgegeben werden können (Xie et al. 2018, 1; Ma et al. 2020, 1). So haben Hitaj et al. (2017) ein aktives Inferenz-Angriffsmodell eingeführt, das originale Trainingsdaten aus dem generierten Datensatz re-konstruieren kann (Xie et al. 2018, 1). Eine mögliche Lösung für die Datenschutzbedenken bietet Differential Privacy (DP). Die Definition von DP kann anhand einer soliden mathema-tischen Formulierung eine Garantie für die Privatsphäre von Teilnehmenden einer Daten-bank schaffen (Torfi et al. 2020, 1). DP wird durch das Hinzufügen von Zufälligkeit in ein System erreicht (Dwork und Roth 2014, 15). Je mehr Zufall einem System beigemengt wird, desto mehr wird die Privatsphäre der einzelnen Teilnehmenden geschützt (Dwork 2011, 338; Torfi et al. 2020, 4). Wird einem Datensatz jedoch mehr Rauschen hinzugefügt, entfernt sich die Verteilung dieses Datensatzes immer weiter von der des originalen Datensatzes. Darun-ter leidet schlussendlich die Effektivität der Daten (vgl. Jordon et al. 2019, 9). Bei Verwen-dung von Differential Privacy gibt es somit immer einen Trade-off zwischen Effektivität der Daten und der erreichten Privatsphäre (Hsu et al. 2014, 1). Dieser Trade-off wird in folgen-der Arbeit anhand eines Beispiels zur Kreditwürdigkeitsprüfung im Finanzwesen empirisch untersucht. Der Aufbau der Analyse folgt dem Referenzprozess für Big Data Analysen nach Müller et al. (2016).


## Abstract

Generative Adversarial Networks (GAN) have recently received increasing attention. One reason is their ability to generate realistic datasets with high privacy protection (Ma et al. 2020, 1). Nevertheless, when applying GANs to sensitive or private training examples, such as financial records, it is a problem that personal information of individuals may be revealed (Xie et al. 2018, 1; Ma et al. 2020, 1). For example, Hitaj et al. (2017) introduced an active inference attack model that can reconstruct original training data from the generated dataset (Xie et al. 2018, 1). Differential privacy (DP) offers a potential solution to privacy concerns. The definition of DP can create a guarantee of privacy for participants in a databank using a solid mathematical formulation (Torfi et al. 2020, 1). DP is achieved by adding randomness to a system (Dwork and Roth 2014, 15). The more randomness added to a system, the more the privacy of individual participants is protected (Dwork 2011, 338; Torfi et al. 2020, 4). However, as more noise is added to a dataset, the distribution of that dataset moves further and further away from that of the original dataset. Ultimately, the effectiveness of the data suffers (cf. Jordon et al. 2019, 9). Thus, when using differential privacy, there is always a trade-off between the effectiveness of the data and the privacy achieved (Hsu et al. 2014, 1). In the following paper, this trade-off is empirically investigated using an example of credit scoring in finance. The structure of the analysis follows the reference process for Big Data analyses according to Müller et al. (2016).



#### Quellenangaben
- Dwork, C. (2011): Differential Privacy. In: van Tilborg, H.; Jajodia, S. (Hg.): Encyclopedia of cryptography and security, 338–340.
- Dwork, C.; Roth, A. (2014): The Algorithmic Foundations of Differential Privacy. In: Foun-dations and Trends® in Theoretical Computer Science, 9 (3-4), 211–407.
- Hitaj, B. et al. (2017): Deep Models Under the GAN: Information Leakage from Collabora-tive Deep Learning. Verfügbar unter: http://arxiv.org/pdf/1702.07464v3.
- Hsu, J. et al. (2014): Differential Privacy: An Economic Method for Choosing Epsilon. Ver-fügbar unter: https://arxiv.org/pdf/1402.3329.
- Jordon, J. et al. (2019): PATE-GAN: Generating Synthetic Data with Differential Privacy Guarantees. In: 7th International Conference on Learning Representations. Verfügbar unter: https://openreview.net/forum?id=S1zk9iRqF7.
- Ma, C. et al. (2020): RDP-GAN: A Rényi-Differential Privacy based Generative Adversarial Network. Verfügbar unter: https://arxiv.org/pdf/2007.02056.
- Müller, O. et al. (2016): Utilizing big data analytics for information systems research: chal-lenges, promises and guidelines. In: European Journal of Information Systems, 25 (4), 289–302.
- Torfi, A. et al. (2020): Differentially Private Synthetic Medical Data Generation using Con-volutional GANs. Verfügbar unter: http://arxiv.org/abs/2012.11774.
- Xie, L. et al. (2018): Differentially Private Generative Adversarial Network. Verfügbar un-ter: https://arxiv.org/pdf/1802.06739.